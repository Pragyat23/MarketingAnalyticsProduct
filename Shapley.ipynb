{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "from numpy import nan\n",
    "import seaborn as sns\n",
    "import  math\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import IPython \n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE,SMOTENC\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, auc, precision_recall_curve\n",
    "\n",
    "import xgboost as xgb\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble  import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import svm\n",
    "# from bentoml import env, artifacts, api, BentoService\n",
    "# from bentoml.adapters import JsonInput, JsonOutput, DataframeInput, DataframeOutput\n",
    "# from bentoml.frameworks.sklearn import SklearnModelArtifact\n",
    "# from bentoml.service.artifacts.common import  PickleArtifact\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @env(infer_pip_packages=True)\n",
    "class Shap():\n",
    "    def shap_imp(self, df):\n",
    "        final_data = df.copy()\n",
    "\n",
    "        #print(final_data)\n",
    "\n",
    "        final_data = final_data.set_index(['patient_gid'])\n",
    "\n",
    "        ## Dataset split\n",
    "\n",
    "        X = final_data.loc[:, final_data.columns != 'conversion_flag']\n",
    "        y = final_data.loc[:, final_data.columns == 'conversion_flag']\n",
    "\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#         print(\"X_train\",X_train)\n",
    "#         print(\"y_train\",y_train)\n",
    "        # define dataset\n",
    "\n",
    "        col_list = X_train.columns\n",
    "        X = X_train\n",
    "        y = y_train\n",
    "\n",
    "        ## LOGISTIC\n",
    "        lr = LogisticRegression(class_weight='balanced')\n",
    "        lr_clf = lr.fit(X_train, y_train)\n",
    "\n",
    "        ### SHAP\n",
    "\n",
    "        coef_importances = pd.Series(lr.coef_[0], index=X_train.columns)\n",
    "\n",
    "        # sort by absolute value, highest to lowest\n",
    "        sorted_by_abs_val = coef_importances.abs().sort_values(ascending=False).index\n",
    "        coef_importances = coef_importances[sorted_by_abs_val]\n",
    "\n",
    "        pred_lr = pd.Series(lr.predict_proba(X_test)[:, 1],\n",
    "                            index=X_test.index)\n",
    "\n",
    "        #%%time\n",
    "        background_data = X_train.sample(X.shape[0], random_state=3)\n",
    "        background_means = shap.kmeans(background_data, k=2)\n",
    "\n",
    "        # SHAP\n",
    "        predictor_function_logreg = lambda x: lr.predict_proba(x)[:, 1]    \n",
    "        explainer = shap.KernelExplainer(predictor_function_logreg, background_means, link='logit')\n",
    "\n",
    "        # take a sample of the dataset, n=1000\n",
    "        selected_indices_logreg = pred_lr.sample(n=X_test.shape[0], random_state=1).index\n",
    "        selected_examples_logreg = X_test.loc[selected_indices_logreg, :]\n",
    "        \n",
    "        ##shap importance \n",
    "        shap_values_logreg = explainer.shap_values(selected_examples_logreg, l1_reg='num_features(15)',matplotlib=True,silent=True)\n",
    "        mean_shap_importances = (pd.DataFrame(shap_values_logreg, columns=selected_examples_logreg.columns).abs().mean().sort_values(ascending=False))\n",
    "        shap_imp = pd.DataFrame(mean_shap_importances)\n",
    "        shap_imp.reset_index(inplace = True)\n",
    "        \n",
    "        ## dataframe with shap importance values\n",
    "        shap_imp.columns =['grain', 'SHAP_Importance']\n",
    "        normalized_sum = sum(shap_imp[\"SHAP_Importance\"])\n",
    "        shap_imp[\"SHAP_Norm_Imp\"] = shap_imp[\"SHAP_Importance\"] / normalized_sum\n",
    "\n",
    "        print(shap_imp)\n",
    "\n",
    "        #return shap_imp\n",
    "\n",
    "#     @api(input=DataframeInput(orient=\"records\"),batch=True,output=DataframeOutput(output_orient='records'))\n",
    "    def predict(self, df):\n",
    "        \n",
    "        shap_imp = self.shap_imp(df)\n",
    "        #shap_imp = shap_imp.reset_index()\n",
    "#         df_final=pd.DataFrame()\n",
    "#         ## model output is returned in a concated form\n",
    "#         df_final['all_columns'] =(shap_imp['grain'].astype(str)+'|'+shap_imp['SHAP_Importance'].astype(str)+'|'+shap_imp['SHAP_Norm_Imp'].astype(str))\n",
    "#         list_zero=['0']*(df.shape[0] - shap_imp.shape[0])\n",
    "#         xtra = {'all_columns': list_zero}\n",
    "#         df_final = df_final.append(pd.DataFrame(xtra))\n",
    "#         return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  grain  SHAP_Importance  SHAP_Norm_Imp\n",
      "0    P4         0.359233       0.511108\n",
      "1    P2         0.213330       0.303521\n",
      "2    P1         0.130288       0.185371\n",
      "3    P3         0.000000       0.000000\n"
     ]
    }
   ],
   "source": [
    "shapley = Shap()\n",
    "df = pd.read_csv(\"./input/NC_SHAP_sample.csv\")\n",
    "output = shapley.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
